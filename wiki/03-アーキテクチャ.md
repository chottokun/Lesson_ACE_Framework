# 🏗️ 3. アーキテクチャ

ACE Frameworkのアーキテクチャは、**同期的な対話ループ**と**非同期的な学習ループ**という2つの主要なサイクルで構成されています。

## システム構成図

```mermaid
graph TD
    subgraph UI ["User Interface (app.py)"]
        User["👤 ユーザー"] -->|"1. メッセージ入力"| Gradio["🌐 Gradio UI"]
    end

    subgraph ACE ["ACE Agent - 同期処理 (agent/graph.py)"]
        Gradio -->|"2. invoke()"| Curator
        Curator["🧠 Curator <br> 意図分析・文脈検索"] -->|"4. 検索"| Memory
        Memory -->|"5. 返却"| Curator
        Curator -->|"6. 文脈注入"| Agent
        Agent["🤖 Agent <br> 応答生成"] -->|"7. 対話内容"| Reflector
        Reflector["📝 Reflector <br> キュー追加"] -->|"8. 保存"| TaskQueue
    end

    subgraph LTM ["Long-Term Memory (memory/)"]
        Memory["📚 長期記憶 <br> (SQLite + FAISS)"]
        TaskQueue["📦 タスクキュー <br> (SQLite)"]
    end

    subgraph BG ["Background Learning - 非同期処理 (workers/)"]
        BG_Worker["⚙️ Background Worker"] -->|"11. タスク取得"| TaskQueue
        TaskQueue -->|"12. タスク渡す"| BG_Worker
        BG_Worker -->|"13. 分析・一般化"| BG_Worker
        BG_Worker -->|"14. 知識保存"| Memory
    end

    Reflector -->|"9. 応答"| Gradio
    Gradio -->|"10. 表示"| User
```

## 1. 同期的な対話ループ (即時応答)
ユーザーのメッセージに対し、リアルタイムで知識を検索し、最適な応答を生成するループです。

1. **Curator**: ユーザーの意図を分析し、長期記憶から関連ドキュメントを抽出。
2. **Agent**: 抽出された知識をコンテキストとして受け取り、LLMで応答を生成。
3. **Reflector**: 対話内容を非同期学習用のタスクキューに登録。

## 2. 非同期的な学習ループ (自己改善)
メインスレッドとは独立して動き、エージェントが過去の対話から継続的に学習するプロセスです。

1. **Task Fetch**: `BackgroundWorker` が未処理のタスクをキューから取得。
2. **Analysis**: 対話を分析し、具体的な解決策と抽象的な一般化（MFR）を抽出。
3. **Synthesis**: 既存の知識を更新すべきか、新規追加すべきかを判断。
4. **Persistence**: 処理された知識を長期記憶（SQLite + FAISS）に反映。

## 技術スタック
- **LLM オーケストレーション**: LangGraph, LangChain
- **ベクトル検索**: FAISS
- **メタデータ管理**: SQLite (FTS5)
- **埋め込みモデル**: sentence-transformers (cl-nagoya/ruri-v3-30m)
- **UI フレームワーク**: Gradio

---
詳細は [コアコンポーネント](./06-コアコンポーネント.md) を参照してください。
